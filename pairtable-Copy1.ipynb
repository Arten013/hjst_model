{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hjst_model.config import DatasetKVSConfig, LayerModelConfig\n",
    "from hjst_model import layers\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nmslib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#カラム内の文字数。デフォルトは50だった\n",
    "pd.set_option(\"display.max_colwidth\", 800)\n",
    "\n",
    "#行数\n",
    "pd.set_option(\"display.max_rows\", 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Sentence\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"tokai\"\n",
    "CONF_DIR = './configs{}'.format('')\n",
    "MODEL_DIR = './results/hjst{}/models/'.format('')\n",
    "layer_conf = LayerModelConfig()\n",
    "layer_conf.link(os.path.join(CONF_DIR, './layer.conf'), create_if_missing=True)\n",
    "layer_conf.set_directory(MODEL_DIR, os.path.join(CONF_DIR, 'dataset.conf'))\n",
    "layer_conf.update_file()\n",
    "\n",
    "for l in ['Sentence']:\n",
    "    print('train', l)\n",
    "    layer_conf.create_layer(dataset_name, layers.TfidfLayer, l, 'tfidf_tokai_spm_8000', tokenizer=\"spm\", vocab_size=8000, lsi_size=500, idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in ['Law', 'Article', 'Sentence']:\n",
    "    print('train', l)\n",
    "    layer_conf.create_layer(dataset_name, layers.SWEMMaxLayer, l, 'swem_concat_tokai_spm_16000', tokenizer=\"spm\", vocab_size=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = layer_conf.load_model(\"swem_max_am_spm_16000-Sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET = \"all\"\n",
    "\n",
    "testset_conf = DatasetKVSConfig()\n",
    "testset_conf.link(\"./configs/test/dataset.conf\")\n",
    "testset_conf.change_section(TESTSET)\n",
    "testset = testset_conf.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare layer config instance\n",
    "layerconf = LayerModelConfig()\n",
    "layerconf.link(\"./configs/layer.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentence layer\n",
    "layerconf.change_section(\"wva_tokai_spm_16000-Sentence\")\n",
    "sentence_layer = layerconf.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load law layer\n",
    "layerconf.change_section(\"wva_tokai_spm_16000-Law\")\n",
    "law_layer = layerconf.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.set_iterator_mode(level=\"Law\", tag=True, sentence=False)\n",
    "law_tags = [tag for tag in testset]\n",
    "law_vectors = np.matrix([law_layer[tag] for tag in law_tags])\n",
    "print(law_vectors.shape)\n",
    "\n",
    "# initialize a new index, using a HNSW index on Cosine Similarity\n",
    "law_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "law_index.addDataPointBatch(law_vectors)\n",
    "law_index.createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "law_neighbours = law_index.knnQueryBatch(law_vectors, k=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.set_iterator_mode(level=\"Sentence\", tag=True, sentence=False)\n",
    "sentence_tags = [tag for tag in testset]\n",
    "sentence_vectors = np.matrix([sentence_layer[tag] for tag in sentence_tags])\n",
    "print(sentence_vectors.shape)\n",
    "\n",
    "# initialize a new index, using a HNSW index on Cosine Similarity\n",
    "sentence_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "sentence_index.addDataPointBatch(sentence_vectors)\n",
    "sentence_index.createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "sentence_neighbours = sentence_index.knnQueryBatch(sentence_vectors, k=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ([ids, distances] for ids, distances in law_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltag_gen = (ltag for ltag in law_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting\n",
    "k = 3\n",
    "\n",
    "# get query law tag\n",
    "query = next(ltag_gen)\n",
    "\n",
    "# get similar law tags and vectors(= simple method output)\n",
    "target_law_indices, _ = law_index.knnQuery(law_layer[query], k=k+1)\n",
    "target_law_tags = [law_tags[i] for i in target_law_indices if law_tags[i] != query][:k]\n",
    "\n",
    "# construct target sentence space\n",
    "target_spaces = dict()\n",
    "target_sentence_tags = dict()\n",
    "for ltag in target_law_tags:\n",
    "    testset.set_iterator_mode(level=\"Sentence\", tag=True, sentence=False)\n",
    "    target_sentence_tags[ltag] = [stag for atag in testset['edges']['Law'][ltag] for stag in testset['edges']['Article'][atag]]\n",
    "    target_sentence_vectors = np.matrix([sentence_layer[tag] for tag in target_sentence_tags[ltag]])\n",
    "\n",
    "    # initialize a new index, using a HNSW index on Cosine Similarity\n",
    "    target_spaces[ltag] = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    target_spaces[ltag].addDataPointBatch(target_sentence_vectors)\n",
    "    target_spaces[ltag].createIndex({'post': 2}, print_progress=True)\n",
    "\n",
    "columns = [str(testset.kvsdicts[\"root\"][os.path.split(tag)[0]])+\"({})\".format(os.path.split(tag)[0]) for tag in [query]+target_law_tags]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "sid = 0\n",
    "for a in testset.kvsdicts[\"edges\"][\"Law\"][query]:\n",
    "    for j, s in enumerate(testset.kvsdicts[\"edges\"][\"Article\"][a]):\n",
    "        sid += 1\n",
    "        df.loc[sid, columns[0]] = testset.kvsdicts[\"texts\"][\"Sentence\"][s]\n",
    "        for l, ltag in enumerate(target_law_tags):\n",
    "            index = target_spaces[ltag]\n",
    "            resi, resd  = index.knnQuery(sentence_layer[s], 1)\n",
    "            df.loc[sid, columns[l+1]] = testset.kvsdicts[\"texts\"][\"Sentence\"][target_sentence_tags[ltag][resi[0]]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 0\n",
    "for a in testset.kvsdicts[\"edges\"][\"Law\"][query]:\n",
    "    for t, s in enumerate(testset.kvsdicts[\"edges\"][\"Article\"][a]):\n",
    "        sid += 1\n",
    "        print(sid, s)#, testset.kvsdicts[\"texts\"][\"Sentence\"][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for the nearest neighbours of the first datapoint\n",
    "ids, distances = next(gen)\n",
    "import os\n",
    "for i in range(10):\n",
    "    tag = law_tags[ids[i]]\n",
    "    print(tag)\n",
    "    print(i, round(distances[i], 3), testset.kvsdicts[\"root\"][os.path.split(tag)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tree.sparse_distance_matrix(tree, 0.6).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
